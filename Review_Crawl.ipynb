{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "875a8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "import concurrent.futures\n",
    "\n",
    "API_KEY = 'e082cc9aab123618a435d30d8c353b03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4935d7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 36s\n",
      "Wall time: 10min 39s\n",
      "CPU times: total: 27.4 s\n",
      "Wall time: 54.9 s\n",
      "CPU times: total: 35.2 s\n",
      "Wall time: 58.6 s\n",
      "CPU times: total: 35 s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SUCHWORT = \"Fussballhandschuhe\"\n",
    "\n",
    "def extract_one_review(url, userlist, reviewlist, starlist, datelist):\n",
    "    params = {'api_key': API_KEY, 'url': url}\n",
    "    \n",
    "    for _ in range(NUM_RETRIES):\n",
    "        try:\n",
    "            response = requests.get('http://api.scraperapi.com/', params=urlencode(params))\n",
    "            if response.status_code in [200, 404]:\n",
    "                ## escape for loop if the API returns a successful response\n",
    "                break\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            response = ''\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "    \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        soupreviews = soup.find(\"div\", class_=\"reviews-content\")\n",
    "    \n",
    "        if soupreviews != None:\n",
    "            reviewblock = soupreviews.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "            for item in reviewblock:\n",
    "                date = item.find(\"span\", {\"data-hook\": \"review-date\"})\n",
    "                if item != None and date.get_text().startswith('Rezension aus Deutschland'):\n",
    "                    user = item.find(\"span\", class_=\"a-profile-name\")\n",
    "                    if user != None:\n",
    "                        userlist.append(user.get_text())\n",
    "                    else:\n",
    "                        userlist.append(' ')\n",
    "                    body = item.find(\"span\", {\"data-hook\": \"review-body\"})\n",
    "                    if body != None:\n",
    "                        reviewlist.append(body.get_text())\n",
    "                    else:\n",
    "                        reviewlist.append( '')\n",
    "                    star = item.find(\"span\", class_=\"a-icon-alt\")\n",
    "                    if star != None:\n",
    "                        starlist.append(star.get_text().split('von')[0][:-3])\n",
    "                    else:\n",
    "                        starlist.append(' ')\n",
    "                    date = item.find(\"span\", {\"data-hook\": \"review-date\"})\n",
    "                    if date != None:\n",
    "                        datelist.append(date.get_text().split('vom')[1][1:])\n",
    "                    else:\n",
    "                        datelist.append(' ')\n",
    "\n",
    "        weiter = soup.find(\"li\", class_=\"a-last\")\n",
    "        if weiter != None:\n",
    "            links = weiter.find(\"a\")\n",
    "        else:\n",
    "            links = None\n",
    "        if links is not None:\n",
    "            link_url = 'https://www.amazon.de' + links[\"href\"]\n",
    "        else:\n",
    "            link_url = ''\n",
    "        return {'user': userlist, 'reviews': reviewlist, 'bewertungen': starlist, 'reviewdates': datelist, 'nexturl': link_url}\n",
    "    \n",
    "def extract_reviews(url):\n",
    "    userlist = []\n",
    "    reviewlist = []\n",
    "    starlist = []\n",
    "    datelist = []\n",
    "    \n",
    "    response = extract_one_review(url, userlist, reviewlist, starlist, datelist)\n",
    "    link_url = response[\"nexturl\"]\n",
    "    \n",
    "    while link_url != '':\n",
    "        response = extract_one_review(link_url, list(response[\"user\"]), list(response[\"reviews\"]), list(response[\"bewertungen\"]), \n",
    "                                      list(response[\"reviewdates\"]))\n",
    "        link_url = response[\"nexturl\"]\n",
    "    \n",
    "    return {'user': response['user'], 'reviews': response['reviews'], 'bewertungen': response['bewertungen'], \n",
    "            'reviewdates': response['reviewdates']}\n",
    "\n",
    "def extract_details(url):\n",
    "    params = {'api_key': API_KEY, 'url': url}\n",
    "    \n",
    "    for _ in range(NUM_RETRIES):\n",
    "        try:\n",
    "            response = requests.get('http://api.scraperapi.com/', params=urlencode(params))\n",
    "            if response.status_code in [200, 404]:\n",
    "                ## escape for loop if the API returns a successful response\n",
    "                break\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            response = ' '\n",
    "            \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        titeltext = soup.find(\"span\", id=\"productTitle\")\n",
    "        if titeltext != None:\n",
    "            titeltext = titeltext.get_text()\n",
    "        else:\n",
    "            titeltext = ' '\n",
    "    \n",
    "        features = soup.find(\"div\", id=\"feature-bullets\")\n",
    "        if features != None:\n",
    "            features  = features.get_text()\n",
    "        else:\n",
    "            features = ' '\n",
    "      \n",
    "        reviewurl = soup.find(\"a\", {\"data-hook\": \"see-all-reviews-link-foot\"})\n",
    "    \n",
    "        if reviewurl != None:\n",
    "            reviewurl = 'https://www.amazon.de' + reviewurl[\"href\"]\n",
    "    \n",
    "        response = extract_reviews(reviewurl)\n",
    "    \n",
    "        erglist.append({'titel': titeltext, 'features': features, 'user': response['user'], \n",
    "            'reviews': response['reviews'], 'bewertungen': response['bewertungen'], 'reviewdates': response['reviewdates']})\n",
    "        \n",
    "\n",
    "\n",
    "NUM_RETRIES = 3\n",
    "NUM_THREADS = 50\n",
    "\n",
    "url_s = 'https://www.amazon.de/s?k=' + SUCHWORT\n",
    "\n",
    "\n",
    "params = {'api_key': API_KEY, 'url': url_s}\n",
    "\n",
    "\n",
    "for _ in range(NUM_RETRIES):\n",
    "        try:\n",
    "            response = requests.get('http://api.scraperapi.com/', params=urlencode(params))\n",
    "            if response.status_code in [200, 404]:\n",
    "                ## escape for loop if the API returns a successful response\n",
    "                break\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            response = ''\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    suchergebnisse = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "    produkt_urls = []\n",
    "    for i in range(len(suchergebnisse)):\n",
    "        produktitel = suchergebnisse[i].find(\"a\")\n",
    "        if produktitel != None:\n",
    "            produkt_urls.append('https://www.amazon.de' + produktitel[\"href\"])\n",
    "\n",
    "erglist = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "    executor.map(extract_details, produkt_urls)\n",
    "\n",
    "suchdf = pd.DataFrame()\n",
    "for i in range(len(erglist)):\n",
    "    temp = pd.DataFrame.from_dict(erglist[i])\n",
    "    suchdf = pd.concat([suchdf, temp], axis=0, ignore_index=True)\n",
    "\n",
    "suchdf.to_csv(SUCHWORT + '.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c7e21a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
